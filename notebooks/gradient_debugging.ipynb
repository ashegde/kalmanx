{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d53791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlx\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from tqdm import tqdm \n",
    "from src.state_space import StateSpaceModel\n",
    "from src.kalman import KalmanFilter\n",
    "from src.utils import sample_mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d189dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f08a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = mx.eye(6)\n",
    "for i in range(3):\n",
    "    F[i, 3+i] = 1 \n",
    "\n",
    "def generate_H(x: Optional[mx.array] = None) -> mx.array:\n",
    "    if x is None:\n",
    "        x = mx.ones((3,))\n",
    "    H = mx.zeros((4, 6))\n",
    "    for i in range(3):\n",
    "        H[i,i] = 1\n",
    "    r = mx.linalg.norm(x)\n",
    "    H[3,3:6]= x/r if r > 0 else x\n",
    "    return H\n",
    "\n",
    "H = generate_H() \n",
    "\n",
    "# Q_true = 0.1 * mx.eye(6)\n",
    "# R_true = mx.diag(mx.array([10000., 10000., 10000., 25.]))\n",
    "Q_true = 0.1 * mx.eye(6)\n",
    "R_true = mx.diag(mx.array([100., 150., 60., 70]))\n",
    "\n",
    "init_state_mean = mx.zeros((6,))\n",
    "init_state_cov = mx.eye(6)\n",
    "\n",
    "truth_model = StateSpaceModel(F, H, Q_true, R_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f048e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(init_state: mx.array,\n",
    "                        model: StateSpaceModel,\n",
    "                        t_steps: int) -> tuple[mx.array, mx.array]:\n",
    "        \n",
    "    process_noise, observation_noise = model.generate_noise(t_steps)\n",
    "\n",
    "    states = []\n",
    "    observations = []\n",
    "\n",
    "    state = init_state\n",
    "    for t in range(t_steps):\n",
    "        states.append(state)\n",
    "        model.update_H(generate_H(state[:3]))\n",
    "        state, observation = model.step(state, process_noise[t], observation_noise[t])\n",
    "        observations.append(observation)\n",
    "        \n",
    "    return mx.stack(states), mx.stack(observations)  \n",
    "\n",
    "\n",
    "def generate_dataset(init_states: mx.array,\n",
    "                     model: StateSpaceModel,\n",
    "                     t_steps: int) -> tuple[mx.array, mx.array]:\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for n in range(init_states.shape[0]):\n",
    "        x, y = generate_trajectory(\n",
    "            init_states[n],\n",
    "            model,\n",
    "            t_steps,\n",
    "        )\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "    return mx.stack(x_train), mx.stack(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68ae823",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "T = 20\n",
    "\n",
    "train_inits = sample_mvn(N, init_state_mean, init_state_cov)\n",
    "test_inits = sample_mvn(N, init_state_mean, init_state_cov)\n",
    "\n",
    "x_train, y_train = generate_dataset(train_inits, truth_model, T)\n",
    "x_test, y_test = generate_dataset(test_inits, truth_model, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f33792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mx.custom_function\n",
    "def linear_solve(A: mx.array, B: mx.array):\n",
    "    \"\"\"\n",
    "    Returns X s.t. AX=B, where A is a symmetric PSD matrix\n",
    "    \"\"\"\n",
    "    return mx.linalg.solve(A, B, stream=mx.Device(mx.cpu))\n",
    "\n",
    "@linear_solve.vjp\n",
    "def linear_solve_vjp(primals, cotangent, output):\n",
    "    A, B = primals\n",
    "    grad_B = mx.linalg.solve(A.T, cotangent, stream=mx.Device(mx.cpu))\n",
    "    grad_A = -grad_B @ output.T\n",
    "    return grad_A, grad_B\n",
    "\n",
    "# @linear_solve.jvp\n",
    "# def linear_solve_jvp(primals, tangents, output):\n",
    "#     A, B = primals\n",
    "#     dA, dB = tangents\n",
    "#     dX = mx.linalg.solve(A, dB - dA @ output, stream=mx.Device(mx.cpu))\n",
    "#     return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb53244",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mx.compile\n",
    "def _kalman_predict(F: mx.array,\n",
    "                   state_mean: mx.array,\n",
    "                   state_cov: mx.array,\n",
    "                   Q: mx.array) -> tuple[mx.array, mx.array]:\n",
    "        state_mean_pred = F @ state_mean\n",
    "        state_cov_pred = F @ state_cov @ F.transpose() + Q\n",
    "        return state_mean_pred, state_cov_pred\n",
    "\n",
    "@mx.compile\n",
    "def _kalman_update(H: mx.array,\n",
    "                  state_mean: mx.array,\n",
    "                  state_cov: mx.array,\n",
    "                  R: mx.array,\n",
    "                  observation: mx.array) -> tuple[mx.array, mx.array]:\n",
    "    \n",
    "    d_y = H.shape[0]\n",
    "    d_x = H.shape[1]\n",
    "\n",
    "    gain = linear_solve( \n",
    "        H @ state_cov @ H.transpose() + R,\n",
    "        H @ state_cov,\n",
    "    )\n",
    "    gain = gain.transpose()\n",
    "    new_state_mean = state_mean + gain @ (observation - H @ state_mean) \n",
    "    new_state_cov = (mx.eye(d_x) - gain @ H) @ state_cov\n",
    "    return new_state_mean, new_state_cov\n",
    "\n",
    "@mx.compile\n",
    "def kalman_step(F: mx.array,\n",
    "                 H: mx.array,\n",
    "                 Q: mx.array,\n",
    "                 R: mx.array,\n",
    "                 state_mean: mx.array,\n",
    "                 state_cov: mx.array,\n",
    "                 observation: mx.array) -> tuple[mx.array, mx.array]:\n",
    "    state_mean, state_cov = _kalman_predict(\n",
    "        F, state_mean, state_cov, Q,\n",
    "    )\n",
    "    estim_state_mean, estim_state_cov = _kalman_update(\n",
    "        H, state_mean, state_cov, R, observation,\n",
    "    )\n",
    "    return estim_state_mean, estim_state_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b6c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mx.compile\n",
    "def cholvec_to_cov(v: mx.array, d: int) -> mx.array:\n",
    "    # cholvec is of dim (1/2 * d (d+1),) for some d\n",
    "    L = mx.zeros((d,d))\n",
    "    for i in range(d):\n",
    "        for j in range(i+1):\n",
    "            index = int(i*(i+1)/2 + j)\n",
    "            L[i,j] = mx.exp(v[index]) if i == j else v[index]\n",
    "    return L @ L.T\n",
    "\n",
    "@mx.compile\n",
    "def cov_to_cholvec(C: mx.array) -> mx.array:\n",
    "    L = mx.linalg.cholesky(C, stream=mx.Device(mx.cpu))\n",
    "    d = C.shape[0]\n",
    "    v = mx.zeros((int(d*(d+1)/2),))\n",
    "    for i in range(d):\n",
    "        for j in range(i+1):\n",
    "            index = int(i*(i+1)/2 + j)\n",
    "            v[index] = mx.log(L[i,j]) if i == j else L[i,j]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91139d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_init = mx.eye(6)\n",
    "R_init = mx.eye(4)\n",
    "\n",
    "model_okf = StateSpaceModel(F, H, Q_init, R_init)\n",
    "okf = KalmanFilter(model_okf, init_state_mean, init_state_cov)\n",
    "\n",
    "def loss_fn(params: dict, x_train: mx.array, y_train: mx.array):\n",
    "    loss = mx.array(0)\n",
    "    Q = cholvec_to_cov(params[\"w_q\"], okf.model.d_x)\n",
    "    R = cholvec_to_cov(params[\"w_r\"], okf.model.d_y)\n",
    "    B, T, _ = x_train.shape\n",
    "    for b in range(B):\n",
    "        state_mean = okf.init_state_mean\n",
    "        state_cov = okf.init_state_cov\n",
    "        for t in range(T):\n",
    "            current_measurement = y_train[b,t,:]  # (d_y,)\n",
    "            okf.update_model_H(current_measurement[:3])\n",
    "            state_mean, state_cov = kalman_step(\n",
    "                okf.model.F, okf.model.H, Q, R,\n",
    "                state_mean, state_cov, current_measurement,\n",
    "            )\n",
    "            loss += mx.square(state_mean - x_train[b, t]).sum()\n",
    "    return loss / (B*T)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"w_q\": cov_to_cholvec(Q_init),\n",
    "    \"w_r\": cov_to_cholvec(R_init),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8c28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import copy\n",
    "\n",
    "# Finite differences are very sensitive to the epsilon value\n",
    "# for small epsilon, the errors blow up. This suggests that \n",
    "# we are doing okay in so far as allowable by floating point.\n",
    "\n",
    "def finite_difference_grads(loss_fn: Callable,\n",
    "                            params: dict,\n",
    "                            x_train: mx.array,\n",
    "                            y_train: mx.array,\n",
    "                            eps: float = 1e-2) -> dict:\n",
    "    \"\"\"Computes finite difference gradient approximation for each parameter.\"\"\"\n",
    "    fd_grads = {}\n",
    "\n",
    "    for key in params:\n",
    "        param = params[key]\n",
    "        grad = mx.zeros_like(param)\n",
    "\n",
    "        # Flatten and iterate over each parameter element\n",
    "        for i in range(param.size):\n",
    "            perturb = mx.zeros_like(param)\n",
    "            perturb[i] = eps\n",
    "\n",
    "            params_plus = copy.deepcopy(params)\n",
    "            params_minus = copy.deepcopy(params)\n",
    "\n",
    "            params_plus[key] = param + perturb\n",
    "            params_minus[key] = param - perturb\n",
    "\n",
    "            # Compute loss at perturbed points\n",
    "            loss_plus = loss_fn(params_plus, x_train, y_train)\n",
    "            loss_minus = loss_fn(params_minus, x_train, y_train)\n",
    "\n",
    "            # Finite difference approximation\n",
    "            grad[i] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "        fd_grads[key] = grad\n",
    "\n",
    "    return fd_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5586ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_q | abs diff: 1.811e-02 | rel error: 2.438e-04\n",
      "w_r | abs diff: 1.493e-02 | rel error: 1.827e-04\n"
     ]
    }
   ],
   "source": [
    "# Choose a small batch to reduce computation time\n",
    "x_sample = x_train\n",
    "y_sample = y_train\n",
    "\n",
    "# Compute autodiff gradients\n",
    "loss, grads = mx.value_and_grad(loss_fn)(params, x_sample, y_sample)\n",
    "\n",
    "# Compute finite difference gradients\n",
    "fd_grads = finite_difference_grads(loss_fn, params, x_sample, y_sample)\n",
    "\n",
    "# Compare gradients\n",
    "for key in grads:\n",
    "    abs_diff = mx.linalg.norm(grads[key] - fd_grads[key])\n",
    "    rel_error = abs_diff / (mx.linalg.norm(grads[key]) + 1e-8)\n",
    "    print(f\"{key} | abs diff: {abs_diff:.3e} | rel error: {rel_error:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
